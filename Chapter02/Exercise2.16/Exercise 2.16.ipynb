{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nlemmatizer = WordNetLemmatizer()","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_text_similarity_jaccard(text1, text2):\n    \"\"\"\n    This method will return Jaccard similarity between two texts\n    after lemmatizing them.\n    :param text1: text1\n    :param text2: text2\n    :return: similarity measure\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    words_text1 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text1)]\n    words_text2 = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text2)]\n    nr = len(set(words_text1).intersection(set(words_text2)))\n    dr = len(set(words_text1).union(set(words_text2)))\n    jaccard_sim = nr / dr\n    return jaccard_sim\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pair1 = [\"What you do defines you\", \"Your deeds define you\"]\npair2 = [\"Once upon a time there lived a king.\", \"Who is your queen?\"]\npair3 = [\"He is desperate\", \"Is he not desperate?\"]","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_text_similarity_jaccard(pair1[0],pair1[1])","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0.14285714285714285"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_text_similarity_jaccard(pair2[0],pair2[1])","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"0.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"extract_text_similarity_jaccard(pair3[0],pair3[1])","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"0.6"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tf_idf_vectors(corpus):\n    tfidf_vectorizer = TfidfVectorizer()\n    tfidf_results = tfidf_vectorizer.fit_transform(corpus).todense()\n    return tfidf_results","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = [pair1[0], pair1[1], pair2[0], pair2[1], pair3[0], pair3[1]]\ntf_idf_vectors = get_tf_idf_vectors(corpus)\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_similarity(tf_idf_vectors[0],tf_idf_vectors[1])","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"array([[0.3082764]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_similarity(tf_idf_vectors[2],tf_idf_vectors[3])","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"array([[0.]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_similarity(tf_idf_vectors[4],tf_idf_vectors[5])","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"array([[0.80368547]])"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}